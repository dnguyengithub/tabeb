<h1 align="center">Tabular data Embedding Benchmark</h1>

<h4 align="center">
    <p>
        <a href="#introduction">Introduction</a> |
        <a href="#quick-start">Quick start</a> |
        <a href="#how-to">How to</a> |
        <a href="#release">Release</a> 
    <p>
</h4>



# Introduction

The Tabular data Embeding Benchmark TabEB is a benchmark for tabular data. 

Tabular data is ominpresent in real-life applications and holds immense potential for insights and innovation. Yet, they are often overlooked. Recently, new studies have shed light on tabular data representation, uncovering its untapped possibilities.

**TabEB** is a comprehensive framework designed to facilitate the building, testing, and benchmarking of methods for tabular data representation. It accommodates a wide range of approaches, from traditional encoding techniques like one-hot encoding to modern foundation model methodologies, enabling researchers and practitioners to explore and optimise the potential of tabular data.

Consider a table (a.k.a datafram) of size *m* x *n* as follows

|  | **Column 1** | **Column 2** | **Column 3** | **...** | **Column n** |
| --- | --- | --- | --- | --- | --- |
| 1 | v_11 | v_12 | v_13 | ... | v_1n |
| 2 | v_21 | v_22 | v_23 | ... | v_2n |
| ... | ... | ... | ... | ... | ... |
| m | v_m1 | v_m1 | v_m1 | ... | v_mn |

Where each column corresponds to a feature, and each row corresponds to an observation. The columns can take data types such as text, numbers, or even images and audio (though these two modalities are not yet supported in this version).

A tabular embedding model (or tabular encoder) encodes each row of the table into a real-valued vector of fixed size, *emb_size*. 

**TabEB** evaluates the performance of tabular encoders based on the performance of downstream models that use the embeddings generated by them. Specifically, we identified the following downstream tasks and their corresponding evaluation protocols::

| Task | Description | Examples of use cases | Supervised vs Unsupervised | Evaluation protocol | Metrics |
| --- | --- | --- | --- | --- | --- |
| Regression | Given the features, estimate the value of the target column (continuous) | All kind of forecasting tasks: - Sales forecasting- Energy consumption prediction | Supervised | The data is split in to train and test. The embedding model embeds both a train and test set. The embeddings from the train set are used to train a *linear regressor*, with a maximum of 100 iterations, and the regressor is evaluated on the test set. | *RMSE* |
| Classification | Given the features, estimate the value of the target column (decrete) | - Assign a category to a product- Label a product- All kind of binary classification tasks: churn forecasting, conversion forecasting, loan default prediction, supervised anolamy detection, etc. | Supervised | The data is split in to train and test. The embedding model embeds both a train and test set. The embeddings from the train set are used to train a logistic regression classifier, with a maximum of 100 iterations, and the classifier is evaluated on the test set. | Micro AUROC (Area Under the Receiver Operating Characteristic curve) |
| Clustering | Given a table, find the cluster of each row | - Customer segmentation: grouping customers into segments based on purchasing behaviour, demographics, and preferences for targeted marketing. | Unsupervised | A mini-batch k-means model, with a batch size of 32 and k set to the number of distinct labels (the labels are provided for evaluation only), is trained on the embeddings of the whole data. | v-measure (https://aclanthology.org/D07-1043.pdf) |
| Ranking | Given a query, find the **k** most relevent rows in the table | - Search engine results- System recommenation- Talent recruitment: ranking job candidates by suitability for a role based on resumes, skills, and experience. | Both | The query and the embeddings are compared using cosinesimilarity. | MAP |
| Unsupervised anomaly detection | Given a table, find the abnormal rows (outlier, duplicates, etc.) | - Fraud detection- Equipment maintenance: detecting unusual sensor readings in machinery that could signal potential failures. | Unsupervised |  | Weighted (micro) F1 score. |
| Unsupervised deduplication | Given a table, find the duplicated rows | - Product dedupliation in e-commerce | Unsupervised | Each pair of the embeddings are compared using cosinesimilarity. A pair is considered as duplicated when the distance is smaller than a threshold | AUC |

Note:

- The choice of linear regression and logistic regression for Regression and Classification tasks, resp. can be controversial, since tree-based models still outperform deep learning on tabular data ([https://arxiv.org/abs/2207.08815](https://arxiv.org/abs/2207.08815)). Our rationale for choosing linear regression and logistic regression is that linear transformations may better reflect the quality of the embeddings. However, these choices are subject to change in the future.
- The relevance of the evaluation metric depends on the specific context of the downstream task. For simplicity, we have chosen the most common metric for each task.

# Quick start

Clone the repo

```
git clone git@github.com:dnguyengithub/tabeb.git
```

Once in the main repo, use poetry to install the dependencies

```
poetry install
```

Example usage

```python
import os
from tabeb.tasks import TabEBRegressionTask, TabEBClassificationTask
from tabeb.interface import get_model, get_leaderboard

# Get the encoders
ROOT_DIR = os.path.join(os.path.dirname(os.getcwd()), "tabeb")
data_dir = os.path.join(ROOT_DIR, "data")
random_encoder = get_model("random_encoder")
carte_encoder = get_model("carte_encoder", task="regression")
skrub_encoder = get_model("skrub_encoder")
model_list = [random_encoder, carte_encoder, skrub_encoder]

# Get the tasks
task_regression = TabEBRegressionTask(data_dir=data_dir)
task_classification = TabEBClassificationTask(data_dir=data_dir)

# Run the tasks
result_dir = os.path.join(ROOT_DIR, "results")
df_regression_scores = task_regression.evaluate(model_list, save_dir=result_dir)
df_classification_scores = task_classification.evaluate(model_list, save_dir=result_dir)

print("***REGRESSION TASK")
display(df_regression_scores)
print("***CLASSIFICATION TASK")
display(df_classification_scores)

# Get the leaderboard
df_leaderboard = get_leaderboard(["regression", "classification"], result_dir)
display(df_leaderboard)

```

Further exploration: see `notebooks/0_quick_start.ipynb`


# How to

## Add a new encoder

To add a new encoder, define this encoder in `tabeb/models/<encoder_name>_encoder.py`

```python
from tabeb.interface import TabEBBaseEncoder

class NewEnvoder(TabEBBaseEncoder):
    """_summary_

    Attributes:
        Any: _description_

    Methods:
        Any: _description_
    """
    def __init__(
        self,
        *args,
        **kwargs
    )
        """_summary_

        Args:
            Any: _description_

        Returns:
            Any: _description_
        """
        ...
    def fit(self, X: np.ndarray, y: np.ndarray = None):
		    """_summary_

        Args:
            Any: _description_

        Returns:
            Any: _description_
        """
        ...
    def transform(self, X: np.ndarray, y: np.ndarray = None):
		    """_summary_

        Args:
            Any: _description_

        Returns:
            Any: _description_
        """
```

**Conditions**

- The encoder must be scikit-learn compatible: it must have a fit and a transform method.
- The output of the transform method, which is the embedding, is fixed size, and does not contain NaN.
- TabEB does not constraint the embedding to be normalised. However, some tasks in TabEB may use metrics that are designed for normalised vectors (e.g. cosine similarity). The performance of the encoder may be impacted by that.

## Add a new task

To add a new task, define the class in `tabeb.tasks.__init__.py` and inherit TabEBBaseTask. In general the only methods to redefine are `get_head()` and `get_score()`

```python
class TabEBNewTask(TabEBBaseTask):
    def __init__(self, **kwarg):
        super().__init__(**kwarg)
        self.task = "task_name"
        self.metric = "metric_name"
        self.data_name_list = ["dataset1", "dataset2"]

    def get_head(self):
        """_summary_

        Returns:
            _type_: _description_
        """
        return LinearRegression()
    
    def get_score(self, y_true: np.array, y_pred: np.array = None) -> float:
        """_summary_

        Args:
            y_true (np.array): _description_
            y_pred (np.array): _description_

        Returns:
            float: _description_
        """
        from sklearn.metrics import root_mean_squared_error
        return root_mean_squared_error(y_true, y_pred)
```

# Release

## Current release

### Available tasks and the corresponding datasets

- **Regression**: wine_pl, wine_vivino_price
- **Classification**: spotify

Datasets to be integrated in the next versions:

- 38 datasets from [https://arxiv.org/abs/2307.14338](https://arxiv.org/abs/2307.14338)
- 10 datastes from [https://arxiv.org/abs/2406.19380](https://arxiv.org/abs/2406.19380)
- 49 datasets from [https://arxiv.org/abs/2402.16785](https://arxiv.org/abs/2402.16785)
- Datasets from [https://archive.ics.uci.edu/](https://archive.ics.uci.edu/)

### Available encoders

- **Random encoder**: generates a random embedding for each row. This encoder serves and a tool to test the functionality of the benchmark.
- **Skrub encoder**: uses Skrub TableVectorizer to transform a dataframe to a numeric (vectorized) representation using classical encoding techniques such as one-hot encoding, datetime encoding, etc. For more details: [https://skrub-data.org/stable/reference/generated/skrub.TableVectorizer.html](https://skrub-data.org/stable/reference/generated/skrub.TableVectorizer.html).
- **CARTE encoder**: we developed a graph-based encoder based on CARTE ([https://arxiv.org/abs/2402.16785](https://arxiv.org/abs/2402.16785)).